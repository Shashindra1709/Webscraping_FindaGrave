# import pandas as pd
# import time
# import re
# from bs4 import BeautifulSoup
# from selenium import webdriver
# from selenium.webdriver.chrome.options import Options
# from selenium.webdriver.common.by import By
# from selenium.webdriver.support.ui import WebDriverWait
# from selenium.webdriver.support import expected_conditions as EC
# from selenium.common.exceptions import TimeoutException, ElementNotInteractableException, NoSuchElementException
# from fake_useragent import UserAgent
# import random


# def scrape_data(first_name, last_name="", location=""):
#     """
#     Scrapes data from Find A Grave website based on the provided parameters.

#     Args:
#         first_name (str): First name of the individual.
#         last_name (str): Last name of the individual (optional).
#         location (str): City of the individual (optional).

#     Returns:
#         list: A list of tuples containing names, birth dates, death dates, and location.
#     """
#     url = "https://www.findagrave.com/memorial/search"

#     # Web driver setup
#     try:
#         chrome_options = Options()
#         chrome_options.add_argument('--no-sandbox')
#         chrome_options.add_argument('--disable-dev-shm-usage')

#         ua = UserAgent()
#         user_agent = ua.random
#         chrome_options.add_argument(f"user-agent={user_agent}")

#         driver = webdriver.Chrome(options=chrome_options)
#         driver.get(url)
#         time.sleep(random.uniform(2, 5))  # Wait for the page to load
#     except Exception as e:
#         print(f"Error initializing the web driver: {e}")
#         return []

#     all_data = []

#     try:
#         # Input the search parameters into the form
#         try:
#             # Input first name
#             first_name_input = WebDriverWait(driver, 10).until(
#                 EC.element_to_be_clickable((By.NAME, "firstname"))
#             )
#             first_name_input.clear()
#             first_name_input.send_keys(first_name)

#             # Input last name
#             if last_name:
#                 last_name_input = driver.find_element(By.NAME, "lastname")
#                 last_name_input.clear()
#                 last_name_input.send_keys(last_name)

#             # Input location into the typeahead input field
#             if location:
#                 location_input = WebDriverWait(driver, 10).until(
#                     EC.element_to_be_clickable((By.ID, "location"))
#                 )

#                 location_input.click()
#                 time.sleep(1)
#                 location_input.clear()
#                 location_input.send_keys(location)

#                 # Wait for the dropdown list to appear and select the first suggestion
#                 try:
#                     suggestion_list = WebDriverWait(driver, 10).until(
#                         EC.visibility_of_element_located((By.XPATH, "//div[@role='listbox']"))
#                     )
#                     first_suggestion = suggestion_list.find_element(By.TAG_NAME, "p")
#                     first_suggestion.click()
#                     print(f"Selected location: {location}")
#                 except NoSuchElementException:
#                     print(f"No suggestions found for location '{location}'. Proceeding without location.")
#                 time.sleep(2)

#         except Exception as e:
#             print(f"Error entering search parameters: {e}")
#             return []

#         # Submit the search form
#         search_button = driver.find_element(By.XPATH, "//button[contains(text(), 'Search')]")
#         search_button.click()
#         time.sleep(5)

#         # Start scraping the results
#         while True:
#             soup = BeautifulSoup(driver.page_source, 'html.parser')

#             # Extract name, date, and location information
#             name_tags = soup.find_all('h2', class_='name-grave')
#             date_tags = soup.find_all('b', class_='birthDeathDates')
#             location_divs = soup.find_all('div', class_='memorial-item---cemet')

#             names, birth_dates, death_dates, locations = [], [], [], []

#             for tag in name_tags:
#                 raw_name = tag.get_text(separator=" ", strip=True)
#                 clean_name = re.sub(r'[“”]', '', raw_name)
#                 clean_name = re.sub(r'[^\w\s,.]', '', clean_name)
#                 clean_name = re.sub(r'\s{2,}', ' ', clean_name).strip()
#                 names.append(clean_name)

#             for tag in date_tags:
#                 date_text = tag.get_text(strip=True)
#                 if date_text == "Birth and death dates unknown.":
#                     birth_dates.append(None)
#                     death_dates.append(None)
#                 else:
#                     birth_date, death_date = [d.strip() for d in date_text.split('–')]
#                     birth_dates.append(birth_date)
#                     death_dates.append(death_date)

#             # Extract location information and clean it
#             for div in location_divs:
#                 location_tag = div.find('p', class_='addr-cemet', string=lambda x: x and 'Plot info:' not in x)
#                 if location_tag:
#                     location_text = " ".join(location_tag.get_text(separator=" ", strip=True).split())
#                     locations.append(location_text if location_text else "Unknown")
#                 else:
#                     locations.append("Unknown")

#             page_data = list(zip(names, birth_dates, death_dates, locations))
#             all_data.extend(page_data)

#             # Pagination or scrolling
#             try:
#                 next_button = driver.find_element(By.LINK_TEXT, "Next")
#                 driver.execute_script("arguments[0].scrollIntoView(true);", next_button)
#                 time.sleep(1)
#                 next_button.click()
#                 time.sleep(5)
#             except NoSuchElementException:
#                 print("No more pages to load. Stopping.")
#                 break

#     finally:
#         driver.quit()

#     return all_data


# def main():
#     try:
#         # Load the data file
#         file_path = "C:\\Users\\USER\\Desktop\\Scraping\\deceased_file.csv"  # Replace with your file's name
#         df = pd.read_csv(file_path)

#         # Extract required columns
#         filtered_df = df[["PartyFirst_Nm", "PartyLast_Nm", "City_Nm"]].dropna().head(5)

#         # Prepare an empty list to store the scraped results
#         all_scraped_data = []

#         # Iterate through each row of the DataFrame
#         for _, row in filtered_df.iterrows():
#             first_name = row["PartyFirst_Nm"]
#             last_name = row["PartyLast_Nm"]
#             location = row["City_Nm"]

#             print(f"Scraping data for: {first_name} {last_name} from {location}...")
#             scraped_data = scrape_data(first_name, last_name, location)

#             if scraped_data:
#                 all_scraped_data.extend(scraped_data)

#             time.sleep(random.uniform(5, 15))  # Avoid overloading the server

#         # Save all scraped data to a CSV file
#         scraped_df = pd.DataFrame(all_scraped_data, columns=["Full Name", "Birth Date", "Death Date", "Location"])
#         scraped_file = "scraped_results.csv"
#         scraped_df.to_csv(scraped_file, index=False, encoding='utf-8')
#         print(f"Scraped data saved to '{scraped_file}'.")

#     except FileNotFoundError as e:
#         print(f"File not found: {e}")
#     except KeyboardInterrupt:
#         print("\nOperation interrupted by user.")


# if __name__ == "__main__":
#     main()



######################################
######################################

import pandas as pd
import time
import re
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, ElementNotInteractableException, NoSuchElementException
from fake_useragent import UserAgent
import random


def scrape_data(first_name, last_name="", location=""):
    """
    Scrapes data from Find A Grave website based on the provided parameters.

    Args:
        first_name (str): First name of the individual.
        last_name (str): Last name of the individual (optional).
        location (str): City of the individual (optional).

    Returns:
        list: A list of tuples containing names, birth dates, death dates, and location.
    """
    url = "https://www.findagrave.com/memorial/search"

    # Web driver setup
    try:
        chrome_options = Options()
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')

        ua = UserAgent()
        user_agent = ua.random
        chrome_options.add_argument(f"user-agent={user_agent}")

        driver = webdriver.Chrome(options=chrome_options)
        driver.get(url)
        time.sleep(random.uniform(2, 5))  # Wait for the page to load
    except Exception as e:
        print(f"Error initializing the web driver: {e}")
        return []

    all_data = []

    try:
        # Input the search parameters into the form
        try:
            # Input first name
            first_name_input = WebDriverWait(driver, 10).until(
                EC.element_to_be_clickable((By.NAME, "firstname"))
            )
            first_name_input.clear()
            first_name_input.send_keys(first_name)

            # Input last name
            if last_name:
                last_name_input = driver.find_element(By.NAME, "lastname")
                last_name_input.clear()
                last_name_input.send_keys(last_name)

            # Input location into the typeahead input field
            if location:
                location_input = WebDriverWait(driver, 10).until(
                    EC.element_to_be_clickable((By.ID, "location"))
                )

                location_input.click()
                time.sleep(1)
                location_input.clear()
                location_input.send_keys(location)

                # Wait for the dropdown list to appear and select the first suggestion
                try:
                    suggestion_list = WebDriverWait(driver, 10).until(
                        EC.visibility_of_element_located((By.XPATH, "//div[@role='listbox']"))
                    )
                    first_suggestion = suggestion_list.find_element(By.TAG_NAME, "p")
                    first_suggestion.click()
                    print(f"Selected location: {location}")
                except NoSuchElementException:
                    print(f"No suggestions found for location '{location}'. Proceeding without location.")
                time.sleep(2)

        except Exception as e:
            print(f"Error entering search parameters: {e}")
            return []

        # Set the death year filter to +/- 3 years
        try:
            # Wait for the dropdown button and make sure it's visible and clickable
            filter_button = WebDriverWait(driver, 10).until(
                EC.element_to_be_clickable((By.ID, "dropdownbyfilter2"))
            )

            # Ensure the button is not blocked by other elements
            driver.execute_script("arguments[0].scrollIntoView(true);", filter_button)
            time.sleep(1)
            driver.execute_script("arguments[0].click();", filter_button)  # Use JS click for better reliability

            # Locate all dropdown items and select the required option
            dropdown_items = driver.find_elements(By.XPATH, "//a[@class='dropdown-item']")
            for item in dropdown_items:
                if item.text.strip() == "+/- 3 years":
                    driver.execute_script("arguments[0].click();", item)  # Use JS click for reliability
                    print("Death year filter set to +/- 3 years.")
                    break
            else:
                print("Error: +/- 3 years option not found.")
        except Exception as e:
            print(f"Error setting death year filter: {e}")

        # Set the death year explicitly
        try:
            # Set a fixed death year value for the search, e.g., 2022
            year_input = WebDriverWait(driver, 10).until(
                EC.element_to_be_clickable((By.NAME, "deathyear"))
            )
            year_input.clear()
            year_input.send_keys("2022")
            print("Death year set to 2022.")
        except Exception as e:
            print(f"Error setting death year: {e}")

        # Submit the search form
        try:
            # Wait for the "SEARCH" button to be clickable and click it
            search_button = WebDriverWait(driver, 10).until(
                EC.element_to_be_clickable((By.XPATH, "//button[@type='submit' and contains(@class, 'btn-primary') and contains(text(), 'SEARCH')]"))
            )
            search_button.click()
            time.sleep(5)
            print("Search button clicked successfully.")
        except TimeoutException:
            print("Error: Search button not found or not interactable.")
            return []

        # Start scraping the results
        while True:
            soup = BeautifulSoup(driver.page_source, 'html.parser')

            # Extract name, date, and location information
            name_tags = soup.find_all('h2', class_='name-grave')
            date_tags = soup.find_all('b', class_='birthDeathDates')
            location_divs = soup.find_all('div', class_='memorial-item---cemet')

            names, birth_dates, death_dates, locations = [], [], [], []

            for tag in name_tags:
                raw_name = tag.get_text(separator=" ", strip=True)
                clean_name = re.sub(r'[“”]', '', raw_name)
                clean_name = re.sub(r'[^\w\s,.]', '', clean_name)
                clean_name = re.sub(r'\s{2,}', ' ', clean_name).strip()
                names.append(clean_name)

            for tag in date_tags:
                date_text = tag.get_text(strip=True)
                if date_text == "Birth and death dates unknown.":
                    birth_dates.append(None)
                    death_dates.append(None)
                else:
                    birth_date, death_date = [d.strip() for d in date_text.split('–')]
                    birth_dates.append(birth_date)
                    death_dates.append(death_date)

            # Extract location information and clean it
            for div in location_divs:
                location_tag = div.find('p', class_='addr-cemet', string=lambda x: x and 'Plot info:' not in x)
                if location_tag:
                    location_text = " ".join(location_tag.get_text(separator=" ", strip=True).split())
                    locations.append(location_text if location_text else "Unknown")
                else:
                    locations.append("Unknown")

            page_data = list(zip(names, birth_dates, death_dates, locations))
            all_data.extend(page_data)

            # Pagination or scrolling
            try:
                next_button = driver.find_element(By.LINK_TEXT, "Next")
                driver.execute_script("arguments[0].scrollIntoView(true);", next_button)
                time.sleep(1)
                next_button.click()
                time.sleep(5)
            except NoSuchElementException:
                print("No more pages to load. Stopping.")
                break

    finally:
        driver.quit()

    return all_data



def main():
    try:
        # Load the data file
        file_path = "C:\\Users\\USER\\Desktop\\Scraping\\deceased_file.csv"  
        df = pd.read_csv(file_path)

        # Extract required columns
        filtered_df = df[["PartyFirst_Nm", "PartyLast_Nm", "City_Nm"]].dropna().head(5)

        # Prepare an empty list to store the scraped results
        all_scraped_data = []

        # Iterate through each row of the DataFrame
        for _, row in filtered_df.iterrows():
            first_name = row["PartyFirst_Nm"]
            last_name = row["PartyLast_Nm"]
            location = row["City_Nm"]

            print(f"Scraping data for: {first_name} {last_name} from {location}...")
            scraped_data = scrape_data(first_name, last_name, location)

            if scraped_data:
                all_scraped_data.extend(scraped_data)

            time.sleep(random.uniform(5, 15))  # Avoid overloading the server
            
            print(f"Total entries extracted: {len(scraped_data)} for {first_name} {last_name} from {location}")

        # Save all scraped data to a CSV file
        scraped_df = pd.DataFrame(all_scraped_data, columns=["Full Name", "Birth Date", "Death Date", "Location"])
        scraped_file = "scraped_results.csv"
        scraped_df.to_csv(scraped_file, index=False, encoding='utf-8')
        print(f"Scraped data saved to '{scraped_file}'.")
        
    except FileNotFoundError as e:
        print(f"File not found: {e}")
    except KeyboardInterrupt:
        print("\nOperation interrupted by user.")


if __name__ == "__main__":
    main()


#############################

"""

Full Name,Birth Date,Death Date,Location
John Francis Brennan Sr. V,24 Jun 1929,23 Apr 2024,"Cumberland, Providence County, Rhode Island"
Wayne G Lambert,Oct 1951,17 Oct 2021,"West Greenwich, Kent County, Rhode Island"
John A Jack Daly V,1 Jan 1930,31 Jul 2020,"Hingham, Plymouth County, Massachusetts"
John Sylvain Daly,30 Sep 1959,1 Feb 2022,"Cotuit, Barnstable County, Massachusetts"
Rose Sylvia Costa,8 Aug 1926,11 Feb 2022,"Fall River, Bristol County, Massachusetts"
Rose Lousararian Costa,28 Aug 1934,9 Sep 2024,"Bedford, Middlesex County, Massachusetts"
Francis Anthony Frank Sendrowski,1946,2 Jun 2020,"Lancaster, Worcester County, Massachusetts"
Robert E Lambert V,15 Oct 1946,15 Jun 2019,"Warren, Bristol County, Rhode Island"
William M Bailey,31 Jan 1963,11 Sep 2020,"Exeter, Washington County, Rhode Island"
Edward Kendall,16 Jul 1963,29 Aug 2021,"North Brookfield, Worcester County, Massachusetts"
Carl A. Johnson Jr.,26 Mar 1936,25 Sep 2021,"Cranston, Providence County, Rhode Island"
Carl Robert Johnson,2 Oct 1946,Jun 2021,"Cranston, Providence County, Rhode Island"
Richard E. Dick Patefield Sr.,1 Apr 1929,9 Nov 2020,"Exeter, Washington County, Rhode Island"
Crescienzo Chris Malafronte V,10 May 1921,4 Dec 2021,"Bristol, Bristol County, Rhode Island"
George Dowd Murray V,15 Jan 1945,18 Sep 2022,"Bourne, Barnstable County, Massachusetts"
George L Bud Murray V,unknown,4 Nov 2021,"Weymouth, Norfolk County, Massachusetts"
CPL George P. Murray V,1 Mar 1932,31 Mar 2019,"Haverhill, Essex County, Massachusetts"
George E. Grayson Jr.,7 Jan 1945,1 Mar 2021,"Exeter, Washington County, Rhode Island"
Sylvia M. Evans,4 Dec 1943,3 Mar 2020,"West Warwick, Kent County, Rhode Island"
James Francis Parker,18 Nov 1943,3 Jul 2021,"Milton, Norfolk County, Massachusetts"
James R. Jimmy Parker,31 Dec 1938,17 Oct 2021,"Stoneham, Middlesex County, Massachusetts"
James S Parker V,24 Mar 1945,27 Mar 2023,"Agawam, Hampden County, Massachusetts"
Robert Anthony Brown V,6 Jun 1955,8 May 2024,"Exeter, Washington County, Rhode Island"
Robert H. Bobby Brownie Brown Jr.,5 Feb 1965,11 Jan 2022,"Cranston, Providence County, Rhode Island"
Robert J Brown,18 Jan 1934,29 Aug 2023,"Exeter, Washington County, Rhode Island"
Robert J. Brown,unknown,15 Mar 2022,"Pawtucket, Providence County, Rhode Island"
Robert Leander Brown V,14 Dec 1929,22 Sep 2023,"Exeter, Washington County, Rhode Island"
Robert R Brown,23 Jul 1935,2 Sep 2024,"Cumberland, Providence County, Rhode Island"
John L. Jack Habershaw,26 Jan 1933,27 Feb 2021,"East Providence, Providence County, Rhode Island"
William Bill White Jr. V,5 Mar 1928,17 Jul 2020,"Bourne, Barnstable County, Massachusetts"
William George White,24 May 1929,25 May 2021,"Tewksbury, Middlesex County, Massachusetts"
William Herbert White,18 Dec 1938,4 Jan 2021,"Kingston, Plymouth County, Massachusetts"
William J White,10 Oct 1932,5 May 2020,"Lynn, Essex County, Massachusetts"
William J. White Jr. V,1949,13 Oct 2023,"Medway, Norfolk County, Massachusetts"
William L Buddy White,1948,24 Dec 2019,"Raynham, Bristol County, Massachusetts"
William Robert Chester Bob White Jr.,22 Feb 1939,20 Aug 2024,"Bourne, Barnstable County, Massachusetts"
William L. Whitehead Jr.,10 Mar 1945,19 Mar 2019,"Agawam, Hampden County, Massachusetts"
John J. Rocha,6 Jul 1943,21 May 2022,"Exeter, Washington County, Rhode Island"
Joseph N. Cormier,8 Aug 1927,6 Apr 2022,"Exeter, Washington County, Rhode Island"
Angèle Harik Haddad,unknown,10 Feb 2023,"Medford, Middlesex County, Massachusetts"
Alfred J. Haddad V,3 Feb 1923,20 Jan 2021,"Pittsfield, Berkshire County, Massachusetts"
Alfred John Haddad,10 Apr 1928,5 Aug 2020,"Boston, Suffolk County, Massachusetts"
Ronald J Baptista,28 Nov 1943,16 Mar 2020,"Taunton, Bristol County, Massachusetts"
Bertrand Paul Duguay,11 May 1952,20 Apr 2020,"Middletown, Newport County, Rhode Island"
Edward T. Healey,25 Jul 1947,19 Sep 2021,"Pawtucket, Providence County, Rhode Island"
Shirley Anne Plumley Wilks V,8 Feb 1928,12 Aug 2021,"Agawam, Hampden County, Massachusetts"
Robert A. Grenier V,19 Nov 1930,29 Mar 2022,"Coventry, Kent County, Rhode Island"
William F Hagan,1926,23 Aug 2019,"Burrillville, Providence County, Rhode Island"


"""

